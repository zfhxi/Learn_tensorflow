{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.简介\n",
    "通过层层堆叠全连接层,保证前一层的输出节点数与当前层的输入节点数匹配，即可堆叠出任意层数的网络。我们把这种由神经元相互连接而成的网络叫做神经网络。如下图所示,通过堆叠4个全连接层,可以获得层数为4的神经网络,由于每层为全连接层,称为全连接网络。\n",
    "\n",
    "![](https://github.com/zfhxi/Learn_tensorflow/blob/master/ch06-%C9%F1%BE%AD%CD%F8/img/05.png?raw=true)\n",
    "\n",
    "设计全连接网络时，遵循约束：\n",
    "隐藏层1的输入节点数需和数据的实际特征长度匹配，每层的输入层节点数与上一层输出节点数匹配，输出层的激活函数和节点数需要根据任务的具体设定进行设计。\n",
    "\n",
    "至于与哪一组超参数是最优的，这需要很多的领域经验知识和大量的实验尝试，或者可以通过AutoML技术搜索出较优设定。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.张量方式实现\n",
    "实现上图的全连接网络："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1)定义参数张量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 隐藏层1\n",
    "w1,b1=tf.Variable(tf.random.truncated_normal([784,256],stddev=0.1)),tf.Variable(tf.zeros([256]))\n",
    "# 隐藏层2\n",
    "w2,b2=tf.Variable(tf.random.truncated_normal([256,128],stddev=0.1)),tf.Variable(tf.zeros([128]))\n",
    "# 隐藏层3\n",
    "w3,b3=tf.Variable(tf.random.truncated_normal([128,64],stddev=0.1)),tf.Variable(tf.zeros([64]))\n",
    "# 输出层\n",
    "w4,b4=tf.Variable(tf.random.truncated_normal([64,10],stddev=0.1)),tf.Variable(tf.zeros([10]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2)前向"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10)\n"
     ]
    }
   ],
   "source": [
    "x=tf.random.truncated_normal([16,28*28],mean=0.5,stddev=0.1)\n",
    "with tf.GradientTape() as tape: # 梯度记录器\n",
    "    # x:[b,28*28]\n",
    "    # 隐藏层1：[b,28*28] -> [b,256]\n",
    "    h1=x@w1+b1\n",
    "    h1=tf.nn.relu(h1)\n",
    "    # 隐藏层2：[b,256] -> [b,128]\n",
    "    h2=h1@w2+b2\n",
    "    h2=tf.nn.relu(h2)\n",
    "    # 隐藏层3：[b,128] -> [b,64]\n",
    "    h3=h2@w3+b3\n",
    "    h3=tf.nn.relu(h3)\n",
    "    # 输出层：[b,64] -> [b,10]\n",
    "    h4=h3@w4+b4\n",
    "print(h4.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "最后一层是否需要添加激活函数通常视具体的任务而定,这里加不加都可以。\n",
    "在使用TensorFlow自动求导功能计算梯度时,需要将前向计算过程放置在`tf.GradientTape()`环境中,从而利用`GradientTape`对象的`gradient()`方法自动求解参数的梯度,并利用`optimizers`对象更新参数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.层方式实现"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n"
     ]
    }
   ],
   "source": [
    "# 导入常用网络层\n",
    "from tensorflow.keras import layers,Sequential\n",
    "fc1=layers.Dense(256,activation=tf.nn.relu) # 隐藏层1\n",
    "fc2=layers.Dense(128,activation=tf.nn.relu) # 隐藏层2\n",
    "fc3=layers.Dense(64,activation=tf.nn.relu) # 隐藏层3\n",
    "fc4=layers.Dense(10,activation=None) # 输出层\n",
    "\n",
    "# 前向\n",
    "x=tf.random.normal([4,28*28])\n",
    "h1=fc1(x)\n",
    "h2=fc2(h1)\n",
    "h3=fc3(h2)\n",
    "h4=fc4(h3)\n",
    "print(h4.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "或者将多个全连接层封装为一个网络类"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n"
     ]
    }
   ],
   "source": [
    "model=Sequential([\n",
    "    layers.Dense(256,activation=tf.nn.relu), # 隐藏层1\n",
    "    layers.Dense(128,activation=tf.nn.relu), # 隐藏层2\n",
    "    layers.Dense(64,activation=tf.nn.relu), # 隐藏层3\n",
    "    layers.Dense(10,activation=None) # 输出层\n",
    "])\n",
    "out=model(x)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.优化目标\n",
    "上面完成了前向计算的一部分，最后一步便是计算误差：\n",
    "\n",
    "${\\cal L}=g(f_\\theta(x),y)$\n",
    "\n",
    "* $f_\\theta(\\cdot)$表示利用\\theta参数化的神经网络模型\n",
    "* $g(\\cdot)$称之为误差函数，描述网络预测值$f_\\theta(x)$与真实标签$y$之间的距离\n",
    "\n",
    "希望通过在训练集$\\mathbb D^\\mathrm{train}$上学习到一组参数$\\theta$使得训练的误差$\\mathcal L$最小：\n",
    "\n",
    "$\\theta^*=\\underbrace{\\rm arg\\ min}_\\theta\\ g(f_\\theta\\lgroup x\\rgroup,y), x\\in\\mathbb D^{\\rm train}$\n",
    "\n",
    "针对该最小优化问题，一般采用误差反向传播算法求解，并利用梯度下降算法迭代更新参数：\n",
    "\n",
    "$\\theta^\\prime=\\theta-\\eta\\cdot\\nabla_\\theta{\\cal L}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "pid=os.getpid()\n",
    "!kill -9 $pid\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}