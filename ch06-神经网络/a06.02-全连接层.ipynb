{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.简介\n",
    "全连接层的核心结构与感知机并没有多大差别。它在感知机的基础上,将不连续的阶跃激活函数换成了其它平滑连续可导的激活函数,并通过堆叠多个网络层来增强网络的表达能力。\n",
    "\n",
    "通过替换感知机的激活函数,同时并行堆叠多个神经元来实现多输入、多输出的网络层结构。如下图所示:\n",
    "\n",
    "![](https://github.com/zfhxi/Learn_tensorflow/blob/master/ch06-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img/04.png?raw=true)\n",
    "\n",
    "并行堆叠了 2 个神经元,即 2 个替换了激活函数的感知机,构成 3 输入节点、2 个输出节点的网络层。其中第一个输出节点的输出为:\n",
    "\n",
    "$o_1=\\sigma(w_{11}\\cdot x_1+w_{21}\\cdot x_2+w_{31}\\cdot x_3+b_1)$\n",
    "\n",
    "第二个节点的输出:\n",
    "\n",
    "$o_2=\\sigma(w_{12}\\cdot x_1+w_{22}\\cdot x_2+w_{32}\\cdot x_3+b_2)$\n",
    "\n",
    "输出向量为$o=[o_1,o_2]$。整个网络层可以通过矩阵关系式表达：\n",
    "\n",
    "$[o_1\\quad o_2]=[x_1\\quad x_2\\quad x_3]@\\left[\\begin{matrix}w_{11}&w_{12}\\\\w_{21}&w_{22}\\\\w_{31}&w_{32}\\end{matrix}\\right]+[b_1\\quad b_2]$\n",
    "\n",
    "即\n",
    "\n",
    "$O=X@W+b$\n",
    "\n",
    "其中输入矩阵$X$的shape定义为$[b,d_{in}]$，$b$为样本数量，$d_{in}$为输入节点数（样本特征长度）；权值矩阵$W$的shape定义为$[d_{in},d_{out}]$，$d_{out}$为输出节点数，偏置向量$b$的shape定义为$[d_{out}]$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "考虑批量并行计算，例如2个样本，$x^{(1)}=[x^{(1)}_1,x^{(1)}_2,x^{(1)}_3]$，$x^{(2)}=[x^{(2)}_1,x^{(2)}_2,x^{(2)}_3]$，则可以方便地将上式推广到批量形式：\n",
    "\n",
    "$\n",
    "\\left[\\begin{matrix}o_1^{(1)}&o_2^{(1)}\\\\o_1^{(2)}&o_2^{(2)}\\end{matrix}\\right]\n",
    "=\\left[\\begin{matrix}x_1^{(1)}&x_2^{(1)}&x_3^{(1)}\\\\x_1^{(2)}&x_2^{(2)}&x_3^{(2)}\\end{matrix}\\right]\n",
    "@\\left[\\begin{matrix}w_{11}&w_{12}\\\\w_{21}&w_{22}\\\\w_{31}&w_{32}\\end{matrix}\\right]\n",
    "+[b1\\quad b2]\n",
    "$\n",
    "\n",
    "其中输出矩阵$O$包含了$b$个样本的输出特征，shape为$[b,d_{out}]$。由于每个输出节点与全部的输入节点相连接，这种网络层称为全连接层(Full-connected Layer)或稠密连接层(Dense Layer)\n",
    "\n",
    "## 1.张量方式实现\n",
    "定义好权值张量$W$和偏置张量$b$，并利用TensorFlow提供的批量矩阵相乘函数`tf.matmul()`即可完成网络层的计算。\n",
    "\n",
    "下面假定:\n",
    "* 输入矩阵X的shape为\\[2,28*28\\]，两个样本\n",
    "* 权值矩阵W的shape为\\[28*28,256\\]\n",
    "* 偏置向量b的shape为\\[256\\]\n",
    "* 输出O的shape为\\[2,256\\]\n",
    "\n",
    "实现如下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n"
     ]
    }
   ],
   "source": [
    "x=tf.random.normal([2,28*28])\n",
    "w1=tf.Variable(tf.random.truncated_normal([28*28,256],stddev=0.1))\n",
    "b1=tf.Variable(tf.zeros([256]))\n",
    "o1=tf.matmul(x,w1)+b1 # 线性变换\n",
    "o1=tf.nn.relu(o1) # 激活函数\n",
    "\n",
    "print(o1.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.层方式实现\n",
    "全连接层有封装好的层实现方式：\n",
    "\n",
    "`layers.Dense(units,activation)`\n",
    "* units: 指定输出节点数\n",
    "* activation: 激活函数类型\n",
    "\n",
    "输入节点数会根据第一次运算时的输入shape确定,同时根据输入、输出节点数自动创建并初始化权值张量W和偏置张量b,因此在新建类Dense实例时,并不会立即创建权值张量W和偏置张量b,而是需要调用`build`函数或者直接进行一次前向计算,才能完成网络参数的创建。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256)\n"
     ]
    }
   ],
   "source": [
    "x=tf.random.normal([4,28*28])\n",
    "from tensorflow.keras import layers\n",
    "# 创建全连接层，指定输出节点数和激活函数\n",
    "fc=layers.Dense(256,activation=tf.nn.relu)\n",
    "h1=fc(x)\n",
    "print(h1.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "获取权值W和偏置张量b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 256)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "W=fc.kernel\n",
    "b=fc.bias\n",
    "print(W.shape)\n",
    "print(b.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "获取网络层待优化（待训练）的参数："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(784, 256) dtype=float32, numpy=\n",
      "array([[ 0.0737713 ,  0.03893939, -0.01091818, ...,  0.04434957,\n",
      "         0.0282714 , -0.0280586 ],\n",
      "       [-0.00280415, -0.02636383,  0.07079422, ..., -0.07259096,\n",
      "         0.02353547,  0.07522345],\n",
      "       [-0.01159611, -0.02654509, -0.07125218, ...,  0.02324271,\n",
      "        -0.03804038, -0.01585384],\n",
      "       ...,\n",
      "       [-0.0557959 ,  0.03258117, -0.07514894, ...,  0.02536245,\n",
      "         0.05416979,  0.01563413],\n",
      "       [ 0.07392903,  0.04835364, -0.03015666, ...,  0.00992661,\n",
      "         0.04085183, -0.00453697],\n",
      "       [ 0.06462882, -0.03370189,  0.01819995, ..., -0.04521985,\n",
      "        -0.06010333, -0.04727587]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "paramaters=fc.trainable_weights\n",
    "print(paramaters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果希望获得所有参数（不仅包含待优化，也包含不需要优化的参数）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(784, 256) dtype=float32, numpy=\n",
      "array([[ 0.0737713 ,  0.03893939, -0.01091818, ...,  0.04434957,\n",
      "         0.0282714 , -0.0280586 ],\n",
      "       [-0.00280415, -0.02636383,  0.07079422, ..., -0.07259096,\n",
      "         0.02353547,  0.07522345],\n",
      "       [-0.01159611, -0.02654509, -0.07125218, ...,  0.02324271,\n",
      "        -0.03804038, -0.01585384],\n",
      "       ...,\n",
      "       [-0.0557959 ,  0.03258117, -0.07514894, ...,  0.02536245,\n",
      "         0.05416979,  0.01563413],\n",
      "       [ 0.07392903,  0.04835364, -0.03015666, ...,  0.00992661,\n",
      "         0.04085183, -0.00453697],\n",
      "       [ 0.06462882, -0.03370189,  0.01819995, ..., -0.04521985,\n",
      "        -0.06010333, -0.04727587]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(256,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "all_params=fc.variables\n",
    "print(all_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "利用网络层类对象进行前向计算时,只需要调用类的`__call__`方法即可,即写成`fc(x)`方式便可,它会自动调用类的`__call__`方法,在`__call__`方法中会自动调用`call`方法,这一设定由TensorFlow框架自动完成,因此用户只需要将网络层的前向计算逻辑实现在`call`方法中即可"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "pid=os.getpid()\n",
    "!kill -9 $pid\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}