{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.导数概念\n",
    "函数输出值增量$\\Delta y$与自变量增量$\\Delta x$的比值在$\\Delta x$趋于0时的极限$a$，如果存在，$a$即为在$x_0$处的导数：<br>\n",
    "$\n",
    "a=\\lim\\limits_{\\Delta x\\to 0}\\frac{\\Delta y}{\\Delta x}=\\lim\\limits_{\\Delta x\\to 0}\\frac{f(x+\\Delta x)-f(x)}{\\Delta x}\n",
    "$\n",
    "\n",
    "导数本身是标量，没有方向。\n",
    "\n",
    "## 2.偏导数概念\n",
    "对于一元函数，自变量$\\Delta x$只有两个方向：$x^+$和$x^-$。导数是函数值沿这两个方向中的一个方向的变化率(两个方向变化率相同)<br>\n",
    "对于多元函数，函数的导数拓展为函数值沿着任意$\\Delta x$方向的变化率。<br>\n",
    "而沿坐标轴的几个方向比较特殊，此时的导数称为偏导数(Partial Derivative)。\n",
    "\n",
    "## 3.梯度下降\n",
    "\n",
    "考虑网络参数集$\\theta=\\{w_1,b_1,w_2,b_2,\\cdots\\}$，在利用梯度下降算法优化网络时，需要求出网络所有的偏导数。因此，关注误差函数输出$\\cal L$沿着自变量$\\theta_i$方向上的导数，即\n",
    "$\\nabla_\\theta{\\cal L}=(\\frac{\\partial{\\cal L}}{\\partial\\theta_1},\\frac{\\partial{\\cal L}}{\\partial\\theta_2},\\frac{\\partial{\\cal L}}{\\partial\\theta_3},\\cdots,\\frac{\\partial{\\cal L}}{\\partial\\theta_n})$<br>\n",
    "按着向量形式更新参数：<br>\n",
    "$\\theta^\\prime=\\theta-\\eta\\cdot\\nabla_\\theta{\\cal L}$<br>\n",
    "向量$\\nabla_\\theta{\\cal L}$称为函数的梯度，其由所有偏导数组成，梯度的方向表示函数值上升最快的方向，梯度的反向则表示函数值下降最快的方向\n",
    "\n",
    "梯度下降算法并不能保证得到全局最优解，主要是目标函数的非凸性造成。如下图：\n",
    "\n",
    "![](https://github.com/zfhxi/Learn_tensorflow/blob/master/ch07-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/img/01.png?raw=true)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}